---
title: "Soilfer: Sampling design guidelines"
author: "Luis Rodriguez Lado, Marcos Angelini"
date: "2024-05-27"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
  pdf_document:
    toc: true
---

# Introduction

The process of Digital Soil Maps requires the acquisition of soil data at point scale. This data includes legacy data - i.e. existing data in the area of interest- and/or gathering representative data across diverse landscapes. This guidelines presents an advanced methodology based in an hierarchical sampling approach to implement an advanced soil sampling strategy within the SoilFER project. This implementation allow a standardized methodology of soil sampling methods within the different areas of the project aligned with previous activities developed within the Soils4Africa project, with similar objectives and activities that those included in SoilFER.

## Objectives and Goals

The core objective of integrating a hierarchical spatial sampling strategy in the SoiFer project is to ensure a comprehensive and environmentally representative distribution of soil sampling points for Digital Soil Mapping. The methodology here proposed consists in an Hierarchical sampling at 3 levels of increasing spatial resolution. Covariate Space Coverage (CSC) is used as a randomized method to select units at the first level of sampling hierarchy, the selection of the so-called 'Primary Sampling Units' (PSUs). The use of CSC at this level enhances the representativeness of the sampling design over the environmental conditions. In particular, the sampling methodology aims to:

-   **Maximize Environmental Representativeness**: Guarantee that soil samples span the full breadth of environmental gradients and spatial variability within the study area. This ensures that the sampling design captures a wide range of environmental conditions and variability.

-   **Optimize Sampling Efficiency**: Strategically place sampling points to capture significant environmental variability with minimal redundancy, thereby optimizing resource use. This strategic approach to sampling location selection reduces logistical efforts, maximizing the effectiveness of fieldwork.

-   **Enable Detailed Environmental Analysis**: Facilitate in-depth exploration of environmental conditions, patterns, and their spatial distributions, grounded in a robust understanding of the covariate space. The collected data is highly relevant for decision-making.

Through this document, we outline the process of soil the sampling design for the SoilFER project, with the example of Honduras. The document provide detailed explanations of the `R code` used in the implementation, along with a rationale for their inclusion. This enables the understanding of the sampling procedure and facilitates the change of parameters requires to implements such approach in other areas, preferentially in the SoilFER project.

# Description of the Sampling Units

The SoiFer sampling design is mostly based on the sampling schema implemented in the project Soil4Africa (<https://www.soils4africa-h2020.eu/>). It is a probabilistic sampling design at three hierarchical spatial levels (Primary, Secondary and Tertiary Sampling Units).

### Primary Sampling Units (PSUs)

-   PSUs constitute the broadest spatial units within the study area and covers the upper hierarchy of spatial sampling. They are defined as 2x2 kilometer areas that contain inside Secondary and Tertiary Sampling Units. In Soil4Africa, PSUs are selected upon a stratified random selection using a map of Farming Systems in Africa as the sampling strata.

The total number of sampled PSUs depends on the total number of samples to collect. Ideally, the number os sampled PSUs should be sufficient to cover the range of environmental gradients present in the study area, often determined through statistical analyses. In SoilFER, the number of PSUs is equal to the total of analysed samples by Secondary Sampling Units and per soil sampling depth. Since the number of SSUs collected by PSU is 4, thus the PSU equals to the total number of analysed samples divided by eight (2 sampling depths. The selection of PSUs is made by CSC as explained before.

### Secondary Sampling Units (SSUs)

-   SSUs offer a finer resolution of sampling within each PSU. Each PSU is divided into 400 regular cells of 100m x 100m (1 hectare), the Secondary Sampling Units. Seven SSUs are randomly selected from each PSU, four of them will be target SSU, where samples will be collected, and three will be saved as replacements. Replacements must be used in a sequential manner (from the first to the third). In the case that any SSUs prove unsuitable for sampling, the next in sequence is assessed until all options in the PSU are exhausted.

### Tertiary Sampling Units (TSUs)

-   TSUs represent the most detailed hierarchical level of sampling, focusing on specific points or small plots within SSUs. Each SSU contains three Tertiary Sampling Units (TSUs) of 25 square meters. Thus, within each SSU there are 400 potential TSUs. Only one TSU per SSU is labeled as target sample, while two are retained as replacement locations. If a TSU is unsuitable for sampling, the first replacement alternative TSU is considered and proceeds in a sequential manner (from the first to the third). If all TSU are unsuitable, the entire SSU is rejected. Soil sampling is strictly conducted at designated TSU locations within a maximum shift of 10 meters of their defined coordinates, with no deviations allowed unless specific conditions apply. Surveyors must document reasons for any rejections of sampling points, noting the current location and describing the circumstances leading to the rejection. This is essential for ensuring the integrity and precision of the sampling process. In addition, since replacement points for TSU and SSU are within the same PSU, this facilitates the collection of samples eliminating the need to travel long distances to collect replacements.

# Deviations from the Soils4Africa Schema

Even if based in the Soils4Africa project, the method here used is slightly modified from the original schema. The stratified random sampling schema in Soils4Africa at the PSU selection level, claims to be a complete probabilistic approach. However, the strata used to build the sampling are coarsely delineated upon a farming system classification raster, with low spatial resolution, making that the strata (defined as homogeneous groups of information, completely different one from each other) are not pure strata but clusters (groups with inner diversity) of farming systems. In addition, the stratified sampling method does not ensure spatial bias in the distribution of the samples, leading to the under sampling of some environments, which is not a suitable situation for Digital Soil Mapping. To avoid that, researchers in Soil4Africa used a 'local pivotal method', which increases the spread of the samples in the area but also compromises the idea of the probabilistic random design.

The main objective of sampling in SoilFer is to capture most of the existing soil variation in the country to help the implementation of DSM activities. This implies that the sampling method should ensure that samples cover a large amount of the existing environmental conditions leading to the soil diversity. With this objective in mind, we replaced the simple stratified random sampling, at PSU level, for a Feature Space Coverage method that facilitates an optimal spread of samples following the variations of the environmental situations. We used the Covariate Space Coverage approach including legacy data, as proposed by Dick Brus (<https://dickbrus.github.io/>). This method ensures an optimal distribution of samples along the existing environmental gradients.

For the remaining hierarchical sampling levels (SSU and TSU), the methodology remains the same as in Soils4Africa.

In summary, the primary driver for the size and number of sampling units at each level is to capture the maximum potential environmental heterogeneity of the study area. This ensures that the sampling design comprehensively captures the range of conditions present. Practical considerations, including accessibility, available resources, and time constraints, must be factored into decisions regarding the size and number of sampling units. This sampling strategy effectively balances the need for comprehensive environmental coverage with the practicalities of field data collection, ensuring that the resulting design is both representative and feasible at field scale.

In practice, the creation of PSUs with its respective SSUs and TSU, can be summarized in the following steps:

-   **Generate PSUs**: Covariates are aggregated and adjusted to a vector grid of 2 km x 2 km. Covariate Space Coverage is used to create clusters using scaled environmental information, legacy data as fixed centers for the clusters and a number of random samples, equal to the number of new sampled sites, as potential cluster centers. Thus, the number of clusters generated is based on the number of legacy points plus the number of new sampling points expected to collect. The algorithm interacts 'n' times, at each iteration, the new infill samples are randomly allocated and the mean squared shortest scaled distance (MSSSD), in the scaled covariate space, is calculated. The retained combination of points correspond to the iteration that minimizes the MSSSD. In addition, alternative PSUs are also defined by repeating the above process while excluding the targeted PSU selected in the first round.

-   **Generate SSUs**: Each PSU is divided into cells of 100 m x 100 m and 7 cells are randomly selected. Four of them are selected as target SSUs and 3 are saved as alternative SSUs within the PSU.

-   **Generate TSUs**: One target and three alternative TSU points are randomly allocated within each SSU. Each TSU represents the central point of a sampling site. However, the full sample consists in a composite of four points within a 'Y' shaped schema, with the bottom pointing to the north, and a distance of 2 meters from the central point to the outer marks in the shape.

# Guidelines for the implementation of the sampling design in R

In this section, the code for the complete implementation of the sampling design is presented. Data covariates, country boundaries and legacy data must be previously allocated in proper shapefile files and stored in the corresponding `data/...` folder. The script is explained in the next steps.

## Setup

The working directory is set to the directory where the actual script is located (`r-scripts`) and moved to the previous directory afterwards. This ensures that the environment is properly set up for subsequent analysis. Packages are defined as a list and then loaded into the system with the `lapply` function.

```{r setup, include=TRUE, warning=FALSE}

## 1 - Set environment and load libraries ===========================

  # Set working directory to source file location
    setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
    setwd("../") # Move wd down to the main folder
    
  # Install synoptReg package from github
    #install.packages("remotes") # Install remotes if not installed
    #remotes::install_github("lemuscanovas/synoptReg")
    
  # List of packages
    packages <- c("sp","terra","raster","sf", "sgsR","entropy", "tripack","tibble",
                  "manipulate","dplyr","synoptReg", "doSNOW","Rfast","fields", "ggplot2")
  
  # Load packages
    invisible(lapply(packages, library, character.only = TRUE))
    rm(packages)
    getwd()

```

## User-defined Variables and Functions

A set of user-defined variables and functions necessary for the sampling design is placed together to facilitate changes in the script that can affect to the overall analysis. It is recommended that the information is structured in a certain manner, placing covariates in the `data/rasters/` folder, shapefiles in `data/shapes/` and other non crucial information in `data/other/`. The results of the sampling design will be saved into `data/results/\`. This make easier to determine the file paths for your input and output data.

Since we are dealing with metric units for the definition of the PSUs, the data must be transformed to a Cartesian Coordinate System defining the area of interest. The EPSG code is used for this goal, and must be defined as a number in the corresponding line of code.

The information on the total number of samples must also be included. In this case, the parameter `nsamples` accounts for the total number of samples that will be analysed in the laboratory. Four SSU will be sampled within each PSU, and two soil depths will be collected at each sampling site (TSU), this makes that the total total number of sampled PSUs (`n.psu`) equals to `nsamples/8`.

```{r paths, include=TRUE}
## 2 - Define variables and functions ===========================

  # Path to data folders
    raster.path <- "data/rasters/"
    shp.path <- "data/shapes/"
    other.path <- "data/other/"
    results.path <- "data/results/"
  
  # Define EPSG system
   epsg <- 32616
  
  # Define the total number of analyzed samples
   nsamples <- 7000
  
  # Define the number of PSUs to sample
    n.psu <- round(nsamples/8)
  
  # Define PSU and SSUs sizes 
    psu_size <- 2000  # (default = 2km x 2 km)
    ssu_size <- 100 # (default = 100m x 100m = 1 ha)
  
  # Define number of target and alternative SSUs at each PSU
    num_primary_ssus <- 4
    num_alternative_ssus <- 3
    
  # Define number of TSUs at each SSU
    number_TSUs <- 3
  
  # Define the number of iterations in the clustering process
    iterations <- 10
    
  # Define the minimum crop percent covering selected PSUs
    percent_crop <- 25
    

```

The sampling design here proposed permits to make use of georeferenced soil legacy data data, if available, and determine the best locations of new samples in order to fill in the non-sampled empty spaces ('infill samples'). Existing sampling units can easily be accommodated in the clustering algorithm, using them as fixed cluster centres. Here, a clustering function from Dick Brus (2023) is used to perform hierarchical clustering of covariate information aiming to provide meaningful environmental groupings for soil information analysis The function can be found at (<https://dickbrus.github.io/SpatialSamplingwithR/kmeans.html>). This allows to make groups of environmental conditions based on the information contained in subsets of it, and defined by the environments at the locations of existing legacy data, together with a random sample of `n.psu` elements, with serve as the basis to determine the amount of clusters to create. In this manner, each point is related to a specific cluster in order to account for the maximum diversity in the territory.

```{r csis_function, include=TRUE}
## 3 - Define Covariate Space Coverage function ===========================

  # Clustering CSC function with fixed legacy data
    CSIS <- function(fixed, nsup, nstarts, mygrd) {
      n_fix <- nrow(fixed)
      p <- ncol(mygrd)
      units <- fixed$units
      mygrd_minfx <- mygrd[-units, ]
      MSSSD_cur <- NA
      for (s in 1:nstarts) {
        units <- sample(nrow(mygrd_minfx), nsup)
        centers_sup <- mygrd_minfx[units, ]
        centers <- rbind(fixed[, names(mygrd)], centers_sup)
        repeat {
          D <- rdist(x1 = centers, x2 = mygrd)
          clusters <- apply(X = D, MARGIN = 2, FUN = which.min) %>% as.factor(.)
          centers_cur <- centers
          for (i in 1:p) {
            centers[, i] <- tapply(mygrd[, i], INDEX = clusters, FUN = mean)
          }
          #restore fixed centers
          centers[1:n_fix, ] <- centers_cur[1:n_fix, ]
          #check convergence
          sumd <- diag(rdist(x1 = centers, x2 = centers_cur)) %>% sum(.)
          if (sumd < 1E-12) {
            D <- rdist(x1 = centers, x2 = mygrd)
            Dmin <- apply(X = D, MARGIN = 2, FUN = min)
            MSSSD <- mean(Dmin^2)
            if (s == 1 | MSSSD < MSSSD_cur) {
              centers_best <- centers
              clusters_best <- clusters
              MSSSD_cur <- MSSSD
            }
            break
          }
        }
      }
      list(centers = centers_best, clusters = clusters_best)
    }

```

The functions runs a number of iterations, for each iteration the algorithm randomly selects a set of infill points while leaving fixed the legacy point data location. Then, a new set of cluster centers is created by combining fixed legacy points with the new selected supplementary points, calculates the distance matrix D between all centers and grid cells and assigns each grid cell to the nearest center, forming clusters. The centers are then updated based on the mean of grid points in each cluster and checks for convergence based on the sum of squared differences between the new and old centers. Once converged, the Mean Squared Standard Deviation (MSSSD) of distances is calculated. If it's the first iteration or MSSSD is lower than the previous best trial, the best centers and clusters are updated. The function return is a list containing the best centers and clusters found during the iterations.

## Load Data

Geospatial data including country boundaries, legacy soil data and raster covariates are loaded in the R environment.These datasets serve as the foundation for the soil sampling design. All dataset are transformed to a common projection system and the raster covariates are summarized into a smaller set of covariates by Principal Component Analysis. Only the raster PC accounting for a cumulative variance explanation of 99% are retained in the new raster set. In this example, we generated legacy data as a random set of 100 samples in the country for demostration purposes and stored in the file 'legacy.shp'. This file must be changed accordingly to take into cosiderations the real soil legacy dataset.

```{r load_data, include=TRUE, warning=FALSE}
## 4 - Load country and legacy data ===========================

  # Load and transform the country boundaries
    country_boundaries <- sf::st_read(file.path(paste0(shp.path,"HND.shp")), quiet=TRUE)
    country_boundaries <- country_boundaries %>%
      st_as_sf() %>% sf::st_transform(crs=epsg)
  
  # Load legacy data
    legacy <- sf::st_read(file.path(paste0(shp.path,"legacy.shp")), quiet=TRUE)
  # Transform coordinates to the common projection system
    legacy <- legacy %>%
      sf::st_transform(crs=epsg)

    
```

Load covariates as raster stack.

```{r load_covs, include=TRUE, message=FALSE, warning=FALSE}
## 5 - Load and transform covariate raster data ===========================

  # Load covariate data
    cov.dat <-  list.files(raster.path, pattern = "covariates_HND.tif$",  recursive = TRUE, full.names = TRUE)
    cov.dat <- terra::rast(cov.dat) # SpatRaster from terra

  # Delete these 2 cov layers because of wrong data for PCA (only if you have them loaded as covariates)
    cov.dat <- cov.dat %>%
      subset(c("dtm_neg_openness_250m","dtm_pos_openness_250m"), negate=TRUE)
    
    # Load soil climate data
    newhall <-  list.files(raster.path, pattern = "newhall.tif$",  recursive = TRUE, full.names = TRUE)
    newhall <- terra::rast(newhall) # SpatRaster from terra
  
  # Merge covs and climate data
    cov.dat <- c(cov.dat, newhall)

  # Project covariates
   cov.dat <- terra::project(cov.dat, paste0("EPSG:", epsg))
  
  # Crop covariates on administrative boundary
   cov.dat <- crop(cov.dat, country_boundaries, mask=TRUE, overwrite=TRUE)
  
  # Simplify raster information with PCA
   pca <- synoptReg::raster_pca(cov.dat) # Faster than with terra::princomp
  
  # Get SpatRaster layers from the pca object
    cov.dat <- pca$PCA
  
  # Subset rasters to main PC (var. explained >= 0.99)
    n_comps <- first(which(pca$summaryPCA[3,] > 0.99))
    cov.dat <- pca$PCA[[1:n_comps]]
  
  # Remove pca stack
    rm(pca)

```

## Load crop data

In this tutorial, the sampling universe is defined by the location of "coffee/african palm/banana/sugarcane" crops. For this, a raster map of such crops is loaded and used as a template to limit the analysis to such crop type. The initial raster resolution of the landuse map is 10m x 10 m. An aggregation of the data by a factor of 10 leads is done to decrease the computation time.

```{r lu_data, include=TRUE, warning=FALSE}}
## 6 - Load Sampling Universe ===========================

  # Load land use data
    lu <- rast(paste0(raster.path,"coffee_2018.tif"))
    names(lu) <- "lu"
  
  # Aggregate to 1 ha pixel size
    lu <- aggregate(lu,10, fun=max, cores=4, na.rm=T)
    lu <- lu/lu  # Ensures that raster has values 1 and NA

```

## Determine PSUs

The determination of Primary Sampling Units (PSUs) is a critical step in the soil sampling design process. In this section, PSUs are identified using covariate space coverage, ensuring representation across different environmental variables.

```{r PSU, include=TRUE}
## 7 - Generate PSUs ===========================

  # Generate 2x2 km PSU vector grid within the country boundaries
    psu_grid <- st_make_grid(country_boundaries, cellsize = c(psu_size, psu_size), square = TRUE)
    psu_grid <- st_sf(geometry = psu_grid)
    psu_grid$ID <- 1:nrow(psu_grid)
    
  # Trim PSU grid by country boundary
    # psu_grid <- psu_grid[country_boundaries[1],] # This process is highly time consuming
    # write_sf(psu_grid,paste0(results.path,"/grid2k.shp")) # Save grid for further analyses

```

The PSU vector grid is cropped by the country boundary and rasterized to 2 km resolution. The new raster stack of covariates is cropped to fit the boundaries of the country PSU grid.

```{r extract_crops, include=TRUE, warning=FALSE}}
## 8 - Select PSUs with crops above a certain percent ===========================

  # Load psu vector grid (saved from previous analyses to save time)
  # If you did the process before, comment next line 
    psu_grid <- sf::st_read(file.path(paste0(other.path,"/grid2k.shp"))) 
    
    # Extract values of lu for cells that intersect with psu_grid
    extracted_values <- terra::extract(lu, psu_grid)
    # Summarize by grid ID and calculate percent according to the 1 ha reclassification of LU (400 pixels in 2x2 km)
    crop_perc <- extracted_values %>%
      group_by(ID) %>%
      summarize(crop_perc = sum(lu, na.rm = TRUE)*100/400)
 
      # Join the result back to the psu_grid polygon data
      psu_grid$crop_perc <- crop_perc$crop_perc
      plot(psu_grid["crop_perc"], col=rainbow(100),border = NA)
      write_sf(psu_grid, file.path(paste0(results.path,"/psu_grid_counts.shp"))) 
      
      # Remove temporary object
      rm(extracted_values)
   
    # Plot the polygons with scaled colors based on crop percentage
    ggplot() +
      geom_sf(data = psu_grid, aes(fill = crop_perc)) +
      scale_fill_distiller(palette = "Spectral") +
      theme_minimal()
    
    # Subset PSUs with a minimal area of crops
      # Since we need 4 target and 3 replacement SSus at each PSU, the crop area in the PSU must have at least 7 has (pixels in aggregated lu layer)
    psu_grid <- psu_grid[psu_grid$crop_perc > percent_crop,"ID"]
  
  # Rasterize PSU vector grid
    template <- rast(vect(psu_grid), res = psu_size)
  # Transfer ID to the rasters
    template <- rasterize(vect(psu_grid), template, field = "ID")


```

The PSU vector grid is rasterized using the raster definition on the landuse map and conserving the corresponding ID as the pixel value. This raster is masked by the landuse map to leave only the PSU raster values on the crop selected areas. A vector identifying the PSU ids within the selected crop is calculated, and used as a selection parameter to subset the vector PSUs that must be used in the calculations.The transformed covariates are 'resampled' to match the extent and resolution of the new created PSU raster grid template.

```{r rasterize_data, include=TRUE}

## 9 - Rasterize PSUs ===========================

  # Using the same resolution and extent as the original raster
    #poly_raster <- rasterize(psu_grid, lu, field="ID", fun="max")  # Assuming 'ID' is a placeholder field
  
  # Identify PSUs overlying raster cells within the selected crop
    #overlay_result <- mask(poly_raster,lu, maskvalue=NA)
  
  # Extract unique PSU IDs within crops (assuming 'ID' is the identifier field in your polygon data)
    #unique_ids <- unique(values(overlay_result)[, "ID"])
  
  # Select PSU Universe (PSUs overlaying coffee cells)
    #psu_grid <- psu_grid[psu_grid$ID %in% unique_ids, ]
  
  # Subset legacy data to the area of crops
    legacy <- st_filter(legacy,psu_grid)
  
  # Plot target area
    # plot(lu)
    # plot(psu_grid, add=TRUE, col='red')
    
  # Crop covariates to the sampling universe
    cov.dat <- crop(cov.dat, psu_grid, mask=TRUE, overwrite=TRUE)
  
  # Resample covariates to the definition of PSUs 
    PSU.r <- resample(cov.dat, template)

```

The selection of the optimal PSUs is based in randomization a random selection of clusters defined with the environmental characteristics at PSU resolution. The clustering and random selection.

```{r csc_psus, include=TRUE}
## 10 - Determine PSUs by Covariate Space Coverage ===========================

  ## Prepare function parameters
  # Convert the raster stack information at PSU aggregated level to a dataframe with coordinates
    PSU.df <- as.data.frame(PSU.r,xy=T)
  
  # Get covariate names
    covs <- names(cov.dat)
  
  # Get the soil legacy coordinates that serve to define fixed cluster centers
    legacy_df <- st_coordinates(legacy)
  
  # Initialize a vector to store the indices of the closest points from legacy data to the dataframe of covariates 'PSU.df' 
    units <- numeric(nrow(legacy_df))
  
  # Loop through each point in 'legacy' to determine the minimum distance between covariates and  legacy points
    for (i in 1:nrow(legacy_df)) {
      # Calculate distances from the current 'legacy' point to all points in the PSUs
      distances <- sqrt((PSU.df$x - legacy_df[i, "X"])^2 + (PSU.df$y - legacy_df[i, "Y"])^2)
      # Find the index of the minimum distance to identify the PSU for the legacy point
      units[i] <- which.min(distances)
    }
  
  # Select scaled information at legacy points
    fixed <- data.frame(units, scale(PSU.df[, covs])[units, ]) 
  
  # Create dataframe of scaled covariatesat the resolution of the PSUs (2x2km)
    mygrd <- data.frame(scale(PSU.df[, covs])) 
  
  ## Compute optimal sampling PSUs considering legacy data
    res <- CSIS(fixed = fixed, nsup = n.psu, nstarts = iterations, mygrd = mygrd)
  
  # Transfer the results to the dataframe of PSUs to identify the cluster for each grid
    PSU.df$cluster <- res$clusters
  
  ## Calculate the distance of the centers to the scaled covariate space
    D <- rdist(x1 = res$centers, x2 = scale(PSU.df[, covs]))
    units <- apply(D, MARGIN = 1, FUN = which.min)
  
  ## Calculate the MSSSD for the selected trial
    dmin <- apply(D, MARGIN = 2, min)
    MSSSD <- mean(dmin^2)
  
  # Subset the selected PSU from all PSU in the country
    myCSCsample <- PSU.df[units, c("x", "y", covs)]
  
  # Identify type of PSU
    myCSCsample$type <- c(rep("legacy", nrow(fixed)), rep("new", length(units)-nrow(fixed)))
  
  # Convert to spatial objet
    myCSCsample <-  myCSCsample %>%
      st_as_sf(coords = c("x", "y"), crs = epsg) 
  
  # Subset legacy and infill PSUs
    legacy <-  myCSCsample[myCSCsample$type=="legacy",] 
    new <-  myCSCsample[myCSCsample$type=="new",] 
  
  # Intersect the PSU grid with the infill data to get the target PSU ids
    PSUs <- sf::st_intersection(psu_grid, new) %>% select(ID)
  
  # Subset target PSUs
    target.PSUs <- psu_grid[psu_grid$ID %in% PSUs$ID,] %>% select(ID)
  
  # Plot of target PSUs
    plot(PSU.r$PC1)
    plot(target.PSUs, col="red", add=TRUE)
    plot(new[1], col="green", pch=19, cex=0.5, add=TRUE)
    plot(legacy[1], col="blue", pch=19, cex=0.5, add=TRUE)

```

The representativeness of the selected PSU can be observed with the plot the selected PSUs over the 1st and 2nd layer of principal components of covariates

```{r selected_PSUs, include=TRUE, warning=FALSE}
## 11 - Plot PSUs over covariate PC1 and PC2 information ===========================

  #  PSUs environmental representation over PC1 and PC2 of covariates
    ggplot(PSU.df) +
      geom_point(mapping = aes(x = PC1, y = PC2, colour = as.character(cluster)), alpha = 0.5) +
      scale_colour_viridis_d() +
      geom_point(data = myCSCsample, mapping = aes(x = PC1, y = PC2), size = .5, colour = "red") +
      scale_x_continuous(name = "PC1") +
      scale_y_continuous(name = "PC2") +
      theme(legend.position = "none") +
      ggtitle("Distribution of sampling PSUs over the space of environmental covariates") 

```

## Determine SSUs and TSUs

Secondary Sampling Units (SSUs) and Tertiary Sampling Units (TSUs) are determined within selected PSUs. This step involves dividing PSUs into smaller units and make a random selection of samples within each PSU and SSUs. We create a function to randomly generate 3 samples within each SSU. This function will be used in the next loop wher generating the SSUs.

```{r SSU1, include=TRUE, warning=FALSE}
## 12 - Function to create SSUs and TSUs ===========================

  # Function to generate 3 TSU points within an SSU, including naming
    generate_tsu_points_within_ssu <- function(ssu, num_tsus, ssu_id, psu_id, ssu_type) {
      bbox <- st_bbox(ssu)
      tsu_points <- vector("list", num_tsus)
      
      for (i in 1:num_tsus) {
        random_x <- runif(1, bbox$xmin, bbox$xmax)
        random_y <- runif(1, bbox$ymin, bbox$ymax)
        tsu_name <- paste("PSU", psu_id, "_", ssu_type, "_SSU", ssu_id, "_TSU", i, sep="")
        tsu_point <- st_point(c(random_x, random_y))
        tsu_points[[i]] <- st_sf(tibble(TSU_Name = tsu_name, TSU_ID = i, SSU_ID = ssu_id, PSU_ID = psu_id, SSU_Type = ssu_type), 
                                 geometry = st_sfc(tsu_point), crs = st_crs(ssu))
      }
      
      tsus_sf <- do.call(rbind, tsu_points)
      return(tsus_sf)
    }

```

We define the size of SSUs and the number of target and alternative SSUs within each PSU and caculate the TSUs with information of SSUs.

```{r SSU2, include=TRUE}
## 13 - Compute SSUs and TSUs ===========================

  # Initialize a list to store TSUs for all PSUs
    all_psus_tsus <- list()
  # Initialize a list to store target SSUS
    selected_ssus <- list()
  
    for (psu_id in 1:nrow(target.PSUs)) {
      selected_psu <- target.PSUs[psu_id, ]
    
    # Generate SSUs within the selected PSU
      ssu_grid <- st_make_grid(selected_psu, cellsize = c(ssu_size, ssu_size), square = TRUE)
      ssu_grid_sf <- st_sf(geometry = ssu_grid)
    
      # Convert ssu_grid_sf to SpatVector
      ssu_grid_vect <- vect(ssu_grid_sf)
      
      # Extract values of lu for cells that intersect with ssu_grid_vect
      extracted_values <- extract(lu, ssu_grid_vect)
      
      # Add lu code to the SSUs        
      ssu_grid_sf$lu <- extracted_values$lu
      
      # Subset ssu_grid_sf to get only the grid squares within lu
      ssu_grid_sf <- ssu_grid_sf[!is.na(ssu_grid_sf$lu), ]
      
      # Count SSUs
      total_ssus <- nrow(ssu_grid_sf)
    
      if(total_ssus >= (num_primary_ssus + num_alternative_ssus)) {
        primary_ssus_indices <- sample(1:total_ssus, num_primary_ssus, replace = FALSE)
        available_for_alternatives <- setdiff(1:total_ssus, primary_ssus_indices)
        alternative_ssus_indices <- sample(available_for_alternatives, num_alternative_ssus, replace = FALSE)
        
        selected_ssus[[psu_id]] <- rbind(ssu_grid_sf[primary_ssus_indices, ], ssu_grid_sf[alternative_ssus_indices, ])
        
        # Generate TSUs for primary SSUs with naming
        primary_tsus <- lapply(primary_ssus_indices, function(index) {
          generate_tsu_points_within_ssu(ssu_grid_sf[index, ], number_TSUs, index, psu_id, "Target")
        })
        
        # Generate TSUs for alternative SSUs with naming
        alternative_tsus <- lapply(alternative_ssus_indices, function(index) {
          generate_tsu_points_within_ssu(ssu_grid_sf[index, ], number_TSUs, index, psu_id, "Alternative")
        })
        
        # Combine all TSUs of the current PSU into one sf object
        all_psus_tsus[[psu_id]] <- do.call(rbind, c(primary_tsus, alternative_tsus))
      } else {
        warning(paste("PSU", psu_id, "does not have enough SSUs for selection. Skipping."))
      }
    }
    
  # Combine TSUs from all PSUs into one sf object
    all_tsus <- do.call(rbind, all_psus_tsus)
    all_tsus$TSU_Type <- "Target"
    all_tsus[all_tsus$TSU_ID >1,"TSU_Type"] <- "Alternative"
    all_tsus$PSU_Type <- "Target"
    all_tsus <- all_tsus %>%
      dplyr::select("TSU_Name","PSU_ID","SSU_ID","TSU_ID","PSU_Type","SSU_Type","TSU_Type","geometry")

```

The next Figure shows the distribution of SSUs and TSUs on the first target PSU.

```{r plot_1st_PSU, include=TRUE, warning=FALSE}}
## 14 - View TSUs ===========================

  # Plot first PSU with target and alternative SSUs
    plot(selected_psu[1], col=NA, reset=FALSE, main="PSU")
    plot(ssu_grid_sf[primary_ssus_indices, ], col="blue", add=TRUE)
    plot(ssu_grid_sf[alternative_ssus_indices, ], col="red", add=TRUE)
    plot(all_tsus[1], col="green", pch=19, cex=0.5, add=TRUE)
    legend("bottomleft", 
           c("Target SSU", "Alternative SSU", "TSUs"), fill=c("blue", "red",  0), border=c("black","black",NA), horiz=F, cex=0.8, pch = c(NA,NA,3), col=c(NA,NA,"green"))

```

## Export Results

The target PSUs with the respective TSUs are saved to shafefile format. The SSUs are not generated since the required information is already stored in the TSUs database.

```{r write_PSU_TSU, include=TRUE, warning=FALSE}}
## 15 - Write PSUs and TSUs ===========================

  # Convert clusters to raster
    dfr <- PSU.df[,c("x","y","cluster")]
    dfr$cluster <- as.numeric(dfr$cluster)    
    dfr <- rasterFromXYZ(dfr)
    crs(dfr) = paste0("epsg:",epsg) # Define the CRS
  
  # Extract cluster ID for the target PSUS
    PSU_cluster.id <- unlist(extract(dfr, target.PSUs))
  
    valid.PSU_clusters <-
      target.PSUs %>% mutate(
        cluster = extract(dfr, target.PSUs, fun = mean, na.rm = TRUE)
      )
  
    all.PSU_clusters <-
      psu_grid %>% mutate(
        cluster = extract(dfr, psu_grid, fun = mean, na.rm = TRUE)
      )
  
  # Write target PSUs to disk
    write_sf(all.PSU_clusters,paste0(results.path,"/PSU_pattern_cl.shp"))
    write_sf(valid.PSU_clusters,paste0(results.path,"/PSUs_target.shp"))
  
  # Export PST areas and TSU points
    write_sf(all_tsus,paste0(results.path,"/TSUs_target.shp")) # Export TSUs
  
  # Write clusters to tiff
   writeRaster(dfr, paste0(results.path,"/clusters.tif"), overwrite=TRUE)

```

## Determine Alternative PSUs

In some cases, alternative PSUs may need to be identified to ensure robustness of the sampling design. This section determines alternative PSUs based on specific criteria, providing flexibility in the sampling approach.

```{r alternative_PSUs, include=TRUE, warning=FALSE}

## 16 -Calculate alternative PSUs ===========================

  # Calculate replacement PSUs
  # Step 1: Exclude elements present in valid.PSU_clusters from all.PSU_clusters
    remaining.PSU_clusters <- all.PSU_clusters %>%
      filter(!(ID %in% valid.PSU_clusters$ID))
  
  # Step 2: Get unique clustMin values from valid.PSU_clusters
    unique_cluster <- distinct(valid.PSU_clusters, cluster)$cluster
  
  # Initialize a vector to store indices of sampled replacements
    sampled_indices <- integer(0)
  
  # Loop through each unique clustMin to find a replacement
    for (clust in unique_cluster) {
      candidates_indices <- which(remaining.PSU_clusters$cluster == clust)
      
      if (length(candidates_indices) > 0) {
        sampled_index <- sample(candidates_indices, size = 1)
        sampled_indices <- c(sampled_indices, sampled_index)
      }
    }
  
  # Use the collected indices to slice the replacements from remaining.PSU_clusters
  # replacements contains a replacement for each unique cluster in valid.PSU_clusters
    replacements <- remaining.PSU_clusters[sampled_indices, ]

```

## Determine SSUs and TSUs for replacement PSUs

Similar to the previous step, SSUs and TSUs are determined within each replacement PSUs. This ensures a complete set of alternative PSUs, SSUS and TSUs in the sampling design.

```{r repacement_SSU_TSU, include=TRUE, warning=FALSE}}
## 17 -  Determine SSUs and TSUs for alternative PSUs=============================

  # Initialize a list to store TSUs for all PSUs
    alt_psus_tsus_sf <- list()
    selected_ssus_sf <- list()
  
    for (psu_id in 1:nrow(replacements)) {
      selected_psu <- replacements[psu_id, ]
    
    # Generate SSUs within the selected PSU
      ssu_grid <- st_make_grid(selected_psu, cellsize = c(ssu_size, ssu_size), square = TRUE)
      ssu_grid_sf <- st_sf(geometry = ssu_grid)
      
      # Convert ssu_grid_sf to SpatVector
      ssu_grid_vect <- vect(ssu_grid_sf)
      
      # Extract values of lu for cells that intersect with ssu_grid_vect
      extracted_values <- extract(lu, ssu_grid_vect)
      
      # Add lu code to the SSUs        
      ssu_grid_sf$lu <- extracted_values$lu
      
      # Subset ssu_grid_sf to get only the grid squares within lu
      ssu_grid_sf <- ssu_grid_sf[!is.na(ssu_grid_sf$lu), ]
      
      # Count SSUs
      total_ssus <- nrow(ssu_grid_sf)
      
      if(total_ssus >= (num_primary_ssus + num_alternative_ssus)) {
        primary_ssus_indices <- sample(1:total_ssus, num_primary_ssus, replace = FALSE)
        available_for_alternatives <- setdiff(1:total_ssus, primary_ssus_indices)
        alternative_ssus_indices <- sample(available_for_alternatives, num_alternative_ssus, replace = FALSE)
      
       selected_ssus_sf[[psu_id]] <- rbind(ssu_grid_sf[primary_ssus_indices, ], ssu_grid_sf[alternative_ssus_indices, ])
      
      # Generate TSUs for target SSUs with naming
        primary_tsus <- lapply(primary_ssus_indices, function(index) {
          generate_tsu_points_within_ssu(ssu_grid_sf[index, ], number_TSUs, index, psu_id, "Target")
        })
      
      # Generate TSUs for alternative SSUs with naming
        alternative_tsus <- lapply(alternative_ssus_indices, function(index) {
          generate_tsu_points_within_ssu(ssu_grid_sf[index, ], number_TSUs, index, psu_id, "Alternative")
        })
      
      # Combine all TSUs of the current PSU into one sf object
        alt_psus_tsus_sf[[psu_id]] <- do.call(rbind, c(primary_tsus, alternative_tsus))
      } else {
        warning(paste("PSU", psu_id, "does not have enough SSUs for selection. Skipping."))
      }
    }

  # Combine TSUs from all PSUs into one sf object
    alt_tsus_combined_sf <- do.call(rbind, alt_psus_tsus_sf)
    alt_tsus_combined_sf$TSU_Type <- "Target"
    alt_tsus_combined_sf[alt_tsus_combined_sf$TSU_ID >1,"TSU_Type"] <- "Alternative"
    alt_tsus_combined_sf$PSU_Type <- "Alternative"
    alt_tsus_combined_sf <- alt_tsus_combined_sf %>%
      dplyr::select("TSU_Name","PSU_ID","SSU_ID","TSU_ID","PSU_Type","SSU_Type","TSU_Type","geometry")

```

```{r save_alt_SSU_TSU, include=TRUE, warning=FALSE}
## 18 - Plot SSUs and TSUs ===========================

  # Plot of alternative PSUs
    plot(cov.dat$PC1, main="Alternative PSUs Distribution")
    plot(country_boundaries[1], col=NA, reset=FALSE, add=TRUE)
    plot(replacements[1], col="red", add=TRUE)
    plot(alt_tsus_combined_sf[1], col="black", pch=19, cex=0.5, add=TRUE)
  
  # Plot first alternative PSU with target and alternative SSUs
    plot(selected_psu[1], col=NA, reset=FALSE, main="Alternative PSU")
    plot(ssu_grid_sf[primary_ssus_indices, ], col="blue", add=TRUE)
    plot(ssu_grid_sf[alternative_ssus_indices, ], col="red", add=TRUE)
    plot(alt_tsus_combined_sf[1], col="green", pch=19, cex=0.5, add=TRUE)
    legend("bottomleft", 
           c("Target SSU", "Alternative SSU", "Alternative TSUs"), fill=c("blue", "red",  0), border=c("black","black",NA), horiz=F, cex=0.8, pch = c(NA,NA,3), col=c(NA,NA,"green"))

```

## Export Results for Alternative PSUs

The results of SSUs and TSUs within alternative PSUs are exported to shapefiles, allowing for further analysis and comparison with the primary sampling design.

```{r save_alt_SSU_TSU, include=TRUE, warning=FALSE}
## 19 - Plot and export SSUs and TSUs ===========================

  # Export to shapefile
    write_sf(replacements,paste0(results.path,"/PSUs_replacements.shp"))
    write_sf(alt_tsus_combined_sf,paste0(results.path,"/TSUs_replacements.shp"))

```
